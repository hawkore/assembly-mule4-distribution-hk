
#IGNITE INSTANCE CONSIFURAGION
#tells if this instance works as client node or acts as a server node of a mule cluster
ignite.this.node.asclient=false
ignite.this.node.gridname=mule_dev_grid

#PERFORMANCE
#Configure internal thread pool
ignite.this.node.publicThreadPoolSize=2
#Configure system thread pool
ignite.this.node.systemThreadPoolSize=2
#Configure stripped thread pool
ignite.this.node.stripedPoolSize=2
#Configure rebalance Thread Pool Size
ignite.this.node.rebalanceThreadPoolSize=1
#Configure checkpointing Thread Pool Size
ignite.this.node.checkpointingThreads=2
#tipical on very stable low-latency network 500ms...
ignite.this.node.failureDetectionTimeout=60000

#AUTO CLUSTER ACTIVATION DELAY IN MILISECONDS WHEN PERSISTENCE IS ENABLED
ignite.this.node.delayForClusterActivation=20000
ignite.this.node.autoRegisterNodeOnBaseTopologyWhenJoin=true

#SEMENTATION DETECTION (Invalid node)
#Segmentation Policy default STOP (Call)
#RESTART_JVM, restart Ignite node. Usefull on server nodes. Note that this will work only if Ignite is started with {@link org.apache.ignite.startup.cmdline.CommandLineStartup} via standard {@code ignite.{sh|bat}} shell script
#STOP, usefull on client nodes. Hawkore internal implementation controls disconnections a make a new one if stopped
#NOOP, it is up to user to implement logic to handle {@link org.apache.ignite.events.EventType#EVT_NODE_SEGMENTED}.
ignite.this.node.segmentationPolicy=STOP
#Segmentation resolve attempts until apply segmentation policy, default 2 attempts
ignite.this.node.segmentationResolveAttempts=5

#ignite config files location
ignite.this.node.config=${mule.home}/conf/cluster

#IGNITE Work directory -  If not provided, the method will use work directory under IGNITE_HOME, if IGNITE_HOME is not provided then system temp directory is used
ignite.this.node.WorkDirectory=${mule.home}/work

#IGNITE NATIVE PERSISTENCE
#enabled or disabled ignite native persistence - only applied is this node acts as a server node
ignite.this.node.persistence.enabled=true
#DEFAULT - Data updates are never lost surviving any OS or process crashes, or power failure
ignite.this.node.persistence.walMode=LOG_ONLY
#Sets a node to the root directory where data and indexes are to. It's assumed the directory is on a separated HDD
ignite.this.node.persistence.storagePath=${ignite.this.node.WorkDirectory}/ignite/persistence
#Sets a path to the directory where WAL is stored. It's assumed the directory is on a separated HDD
ignite.this.node.persistence.walPath=${ignite.this.node.WorkDirectory}/ignite/wal
#Sets a path to the directory where WAL archive is stored. The directory is on the same HDD as the WAL.
ignite.this.node.persistence.walArchivePath=${ignite.this.node.WorkDirectory}/ignite/archive


#IGNITE SECURITY - Cluster secured comunication betwwen nodes
ignite.this.node.keystore.path=${ignite.this.node.config}/ssl/ignite/keystore/ignite-server.jks
ignite.this.node.keystore.pass=Ape8gZYheTQ6q85siWTs
ignite.this.node.truststore.path=${ignite.this.node.config}/ssl/ignite/keystore/ignite-trust.jks
ignite.this.node.truststore.pass=Ape8gZYheTQ6q85siWTs

#IGNITE TCP discovery SPI: MulticastIpFinder, S3IpFinder... or another configured one
ignite.this.node.ipfinder.bean=localOnly
ignite.this.node.ipfinder.localport=45500
ignite.this.node.ipfinder.portrange=4
ignite.this.node.ipfinder.multicastGroup=224.0.0.1

#IGNITE IP FINDER - AWS S3 based
#on dockerized dev enviroment point to local S3 server s3.docker.test with docker run --link
#on production point to amazon S3 server aws.s3.endpoint=s3.amazonaws.com
ignite.this.node.ipfinder.s3.endpoint=http://s3.docker.test:8000
ignite.this.node.ipfinder.s3.accessKey=accessKey1
ignite.this.node.ipfinder.s3.secretKey=verySecretKey1
#on dev enviroment we have a shared ignite cluster for all grids  
ignite.this.node.ipfinder.s3.bucketName=ipfinder.ignite.mule.cluster.dev
#use path style (https://s3.amazonaws.com/my_bucket) instead of subdomain style (https://my_bucket.s3.amazonaws.com) 
ignite.this.node.ipfinder.s3.withPathStyleAccess=true


